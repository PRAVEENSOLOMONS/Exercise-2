{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_arr = Variable(torch.Tensor([[0,0],[0,1],[1,0],[1,1]]),requires_grad=False)\n",
    "x_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_arr = Variable(torch.Tensor([[0],[1],[1],[0]]),requires_grad=True)\n",
    "y_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class xor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(xor,self).__init__()\n",
    "        self.linear=nn.Linear(2,1)\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        res1=self.linear(x)\n",
    "        res2=self.sigmoid(res1)\n",
    "        return res2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(x.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.29769110679626465\n",
      "Epoch 10, Loss: 0.29769110679626465\n",
      "Epoch 20, Loss: 0.29769110679626465\n",
      "Epoch 30, Loss: 0.29769110679626465\n",
      "Epoch 40, Loss: 0.29769110679626465\n",
      "Epoch 50, Loss: 0.29769110679626465\n",
      "Epoch 60, Loss: 0.29769110679626465\n",
      "Epoch 70, Loss: 0.29769110679626465\n",
      "Epoch 80, Loss: 0.29769110679626465\n",
      "Epoch 90, Loss: 0.29769110679626465\n",
      "Epoch 100, Loss: 0.29769110679626465\n",
      "Epoch 110, Loss: 0.29769110679626465\n",
      "Epoch 120, Loss: 0.29769110679626465\n",
      "Epoch 130, Loss: 0.29769110679626465\n",
      "Epoch 140, Loss: 0.29769110679626465\n",
      "Epoch 150, Loss: 0.29769110679626465\n",
      "Epoch 160, Loss: 0.29769110679626465\n",
      "Epoch 170, Loss: 0.29769110679626465\n",
      "Epoch 180, Loss: 0.29769110679626465\n",
      "Epoch 190, Loss: 0.29769110679626465\n",
      "Epoch 200, Loss: 0.29769110679626465\n",
      "Epoch 210, Loss: 0.29769110679626465\n",
      "Epoch 220, Loss: 0.29769110679626465\n",
      "Epoch 230, Loss: 0.29769110679626465\n",
      "Epoch 240, Loss: 0.29769110679626465\n",
      "Epoch 250, Loss: 0.29769110679626465\n",
      "Epoch 260, Loss: 0.29769110679626465\n",
      "Epoch 270, Loss: 0.29769110679626465\n",
      "Epoch 280, Loss: 0.29769110679626465\n",
      "Epoch 290, Loss: 0.29769110679626465\n",
      "Epoch 300, Loss: 0.29769110679626465\n",
      "Epoch 310, Loss: 0.29769110679626465\n",
      "Epoch 320, Loss: 0.29769110679626465\n",
      "Epoch 330, Loss: 0.29769110679626465\n",
      "Epoch 340, Loss: 0.29769110679626465\n",
      "Epoch 350, Loss: 0.29769110679626465\n",
      "Epoch 360, Loss: 0.29769110679626465\n",
      "Epoch 370, Loss: 0.29769110679626465\n",
      "Epoch 380, Loss: 0.29769110679626465\n",
      "Epoch 390, Loss: 0.29769110679626465\n",
      "Epoch 400, Loss: 0.29769110679626465\n",
      "Epoch 410, Loss: 0.29769110679626465\n",
      "Epoch 420, Loss: 0.29769110679626465\n",
      "Epoch 430, Loss: 0.29769110679626465\n",
      "Epoch 440, Loss: 0.29769110679626465\n",
      "Epoch 450, Loss: 0.29769110679626465\n",
      "Epoch 460, Loss: 0.29769110679626465\n",
      "Epoch 470, Loss: 0.29769110679626465\n",
      "Epoch 480, Loss: 0.29769110679626465\n",
      "Epoch 490, Loss: 0.29769110679626465\n",
      "Epoch 500, Loss: 0.29769110679626465\n",
      "Epoch 510, Loss: 0.29769110679626465\n",
      "Epoch 520, Loss: 0.29769110679626465\n",
      "Epoch 530, Loss: 0.29769110679626465\n",
      "Epoch 540, Loss: 0.29769110679626465\n",
      "Epoch 550, Loss: 0.29769110679626465\n",
      "Epoch 560, Loss: 0.29769110679626465\n",
      "Epoch 570, Loss: 0.29769110679626465\n",
      "Epoch 580, Loss: 0.29769110679626465\n",
      "Epoch 590, Loss: 0.29769110679626465\n",
      "Epoch 600, Loss: 0.29769110679626465\n",
      "Epoch 610, Loss: 0.29769110679626465\n",
      "Epoch 620, Loss: 0.29769110679626465\n",
      "Epoch 630, Loss: 0.29769110679626465\n",
      "Epoch 640, Loss: 0.29769110679626465\n",
      "Epoch 650, Loss: 0.29769110679626465\n",
      "Epoch 660, Loss: 0.29769110679626465\n",
      "Epoch 670, Loss: 0.29769110679626465\n",
      "Epoch 680, Loss: 0.29769110679626465\n",
      "Epoch 690, Loss: 0.29769110679626465\n",
      "Epoch 700, Loss: 0.29769110679626465\n",
      "Epoch 710, Loss: 0.29769110679626465\n",
      "Epoch 720, Loss: 0.29769110679626465\n",
      "Epoch 730, Loss: 0.29769110679626465\n",
      "Epoch 740, Loss: 0.29769110679626465\n",
      "Epoch 750, Loss: 0.29769110679626465\n",
      "Epoch 760, Loss: 0.29769110679626465\n",
      "Epoch 770, Loss: 0.29769110679626465\n",
      "Epoch 780, Loss: 0.29769110679626465\n",
      "Epoch 790, Loss: 0.29769110679626465\n",
      "Epoch 800, Loss: 0.29769110679626465\n",
      "Epoch 810, Loss: 0.29769110679626465\n",
      "Epoch 820, Loss: 0.29769110679626465\n",
      "Epoch 830, Loss: 0.29769110679626465\n",
      "Epoch 840, Loss: 0.29769110679626465\n",
      "Epoch 850, Loss: 0.29769110679626465\n",
      "Epoch 860, Loss: 0.29769110679626465\n",
      "Epoch 870, Loss: 0.29769110679626465\n",
      "Epoch 880, Loss: 0.29769110679626465\n",
      "Epoch 890, Loss: 0.29769110679626465\n",
      "Epoch 900, Loss: 0.29769110679626465\n",
      "Epoch 910, Loss: 0.29769110679626465\n",
      "Epoch 920, Loss: 0.29769110679626465\n",
      "Epoch 930, Loss: 0.29769110679626465\n",
      "Epoch 940, Loss: 0.29769110679626465\n",
      "Epoch 950, Loss: 0.29769110679626465\n",
      "Epoch 960, Loss: 0.29769110679626465\n",
      "Epoch 970, Loss: 0.29769110679626465\n",
      "Epoch 980, Loss: 0.29769110679626465\n",
      "Epoch 990, Loss: 0.29769110679626465\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    y_pred = x(x_arr)\n",
    "    loss_func = criterion(y_arr,y_pred)\n",
    "    optimizer.zero_grad()\n",
    "    loss_func.backward()\n",
    "\n",
    "    if (i%10==0):\n",
    "        print(f'Epoch {i}, Loss: {loss_func.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7433cbdc30d13fed05b4b4d42951820fdebc4db49cfb198a3aaa6b44f3c50e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
